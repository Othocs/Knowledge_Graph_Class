# TP6: Finetune Llama 3.2 on Medical Dataset with Hugging Face and PEFT

This notebook demonstrates how to fine-tune Llama 3.2-1B-Instruct on a medical Q&A dataset using LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.

## Repository Structure

```
TP6/
├── main_colab_final.ipynb    # Main notebook (run on Google Colab with T4 GPU)

├── evaluation_results.json   # Evaluation metrics and predictions
├── results/                  # Training checkpoints
│   ├── checkpoint-300/
│   └── checkpoint-375/
├── llama3_medical_lora/      # Final fine-tuned LoRA adapter
└── README.md
```

## Output Explanation

This section explains the purpose and contents of the main output artifacts generated by this notebook:

### results folder

This folder is generated by the Hugging Face Trainer during the fine-tuning process. It contains training checkpoints, which are snapshots of the model's state at various points during training (e.g., checkpoint-300, checkpoint-375). These checkpoints typically include:
- The model's LoRA adapter weights at that specific training step
- Optimizer and learning rate scheduler states
- trainer_state.json which logs training progress and metrics
- Configuration files like adapter_config.json, tokenizer_config.json, and training_args.bin

### llama3_medical_lora folder

This folder contains the final fine-tuned LoRA adapter weights and the tokenizer after the training process is complete. Specifically, it holds:
- adapter_model.safetensors: The learned LoRA weights that are applied on top of the base Llama 3.2 model
- adapter_config.json: The configuration used for the LoRA setup (e.g., r, lora_alpha, target_modules)
- tokenizer.json, tokenizer_config.json, special_tokens_map.json, chat_template.jinja: These files are essential for loading and correctly using the tokenizer that was paired with the fine-tuned model, ensuring consistent tokenization during inference

### evaluation_results.json file

This JSON file stores a summary of the model's performance on the selected test set. It includes:
- accuracy, exact_matches, partial_matches, incorrect: Overall evaluation metrics
- total_time: The time taken for the evaluation process
- selected_indices: A list of the specific indices from the test set that were used for evaluation
- results: A detailed list of each test example, including the question, ground_truth answer, the model's prediction, and whether it was correct (along with the match_type, e.g., "exact" or "partial")
