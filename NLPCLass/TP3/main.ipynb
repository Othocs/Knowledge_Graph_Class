{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fe72cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install transformers torch sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ii8mm8tu1b",
   "metadata": {},
   "source": [
    "## Question 1: Understanding Pipelines\n",
    "\n",
    "### 1. What is a `pipeline` in Hugging Face Transformers? What does it abstract away from the user?\n",
    "\n",
    "A pipeline is a high-level inference API that makes it easy to use pretrained models. It handles all the complexity behind the scenes: tokenizing the input, running the model, and converting outputs to readable results. You just specify the task and provide data.\n",
    "\n",
    "### 2. List at least 3 other tasks (besides text-classification) available in pipelines:\n",
    "\n",
    "- `text-generation` for generating text from a prompt\n",
    "- `automatic-speech-recognition` for transcribing audio\n",
    "- `image-classification` for classifying images\n",
    "- `question-answering` for extracting answers from context\n",
    "\n",
    "### 3. What happens when you don't specify a model? How can you specify a specific model?\n",
    "\n",
    "Without a model, the pipeline uses a default pretrained model for that task. For `text-classification`, it defaults to `distilbert-base-uncased-finetuned-sst-2-english`.\n",
    "\n",
    "To specify a model, pass the `model` parameter with a Hugging Face Hub model identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "yybzgfh6f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/othocs/Desktop/AIDAMS/Semester 1/knowledgeGraph/Knowledge_Graph_Class/NLPCLass/TP3/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model: [{'label': 'POSITIVE', 'score': 0.9998855590820312}]\n",
      "Custom model: [{'label': '5 stars', 'score': 0.9135047197341919}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier_default = pipeline(\"text-classification\")\n",
    "\n",
    "classifier_custom = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "text = \"I love this product!\"\n",
    "print(\"Default model:\", classifier_default(text))\n",
    "print(\"Custom model:\", classifier_custom(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451293f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kptdoizi52b",
   "metadata": {},
   "source": [
    "## Question 2: Text Classification Deep Dive\n",
    "\n",
    "### 1. What is the default model used for text-classification?\n",
    "\n",
    "`distilbert-base-uncased-finetuned-sst-2-english`\n",
    "\n",
    "You can find it on the [Hugging Face Hub](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
    "\n",
    "### 2. What dataset was this model fine-tuned on? What kind of text does it work best with?\n",
    "\n",
    "The model was fine-tuned on the Stanford Sentiment Treebank (SST-2), which is part of the GLUE benchmark. It works best with short to medium-length English text (up to 128 tokens), especially movie reviews and general sentiment analysis.\n",
    "\n",
    "### 3. What does the `score` field represent? What range of values can it have?\n",
    "\n",
    "The score represents the model's confidence in its prediction, ranging from 0 to 1. A score of 0.90 means the model is 90% confident in that label.\n",
    "\n",
    "### 4. Challenge: Emotion classification model\n",
    "\n",
    "I found `j-hartmann/emotion-english-distilroberta-base`, which classifies text into 7 emotions: anger, disgust, fear, joy, neutral, sadness, and surprise.\n",
    "\n",
    "Link: https://huggingface.co/j-hartmann/emotion-english-distilroberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ihg27942c7j",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I'm so happy today!' -> joy (95.24%)\n",
      "'That made me super angry.' -> anger (97.02%)\n",
      "'Damn, that is so scary!' -> fear (92.46%)\n",
      "'I feel so sad and lonely.' -> sadness (98.68%)\n"
     ]
    }
   ],
   "source": [
    "emotion_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "test_texts = [\n",
    "    \"I'm so happy today!\",\n",
    "    \"That made me super angry.\",\n",
    "    \"Damn, that is so scary!\",\n",
    "    \"I feel so sad and lonely.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = emotion_classifier(text)\n",
    "    print(f\"'{text}' -> {result[0]['label']} ({result[0]['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tr74ax04hq",
   "metadata": {},
   "source": [
    "## Question 3: Named Entity Recognition (NER)\n",
    "\n",
    "### 1. What does the `aggregation_strategy=\"simple\"` parameter do?\n",
    "\n",
    "It groups tokens with the same entity type together based on BIO tags. Without it, you get individual token predictions. With \"simple\", tokens like B-PER, I-PER, I-PER get merged into a single entity, and subword tokens are combined into complete words.\n",
    "\n",
    "### 2. What do the entity types mean? (ORG, MISC, LOC, PER)\n",
    "\n",
    "PER is for person names, LOC for locations, ORG for organizations, and MISC for things that don't fit the other categories (like product names or events).\n",
    "\n",
    "### 3. Why do some words appear with `##` prefix (like `##tron` and `##icons`)?\n",
    "\n",
    "The `##` prefix indicates subword tokens. When a word isn't in the tokenizer's vocabulary, it gets split into smaller pieces. For example, \"Megatron\" becomes \"Mega\" + \"##tron\". The `##` means it's a continuation of the previous token.\n",
    "\n",
    "### 4. Why are \"Megatron\" and \"Decepticons\" split incorrectly?\n",
    "\n",
    "The model was trained on Reuters news data from 1996-1997 (CoNLL-2003 dataset). Fictional character names from Transformers weren't common in news articles back then, so the model doesn't recognize them as coherent entities. This shows that NER models work best on entity types similar to their training data.\n",
    "\n",
    "### 5. Challenge: What is the CoNLL-2003 dataset?\n",
    "\n",
    "CoNLL-2003 is a benchmark NER dataset built from Reuters news stories (Aug 1996 - Aug 1997). It uses IOB2 tagging format with four entity types: PER, LOC, ORG, and MISC. It's available in English and German and is the standard benchmark for evaluating NER models.\n",
    "\n",
    "Model card: https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "jfzqlqy46k",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimus Prime -> MISC (98.33%)\n",
      "Amazon -> ORG (99.39%)\n",
      "Germany -> LOC (99.96%)\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text = \"Last week I ordered an Optimus Prime action figure from Amazon in Germany.\"\n",
    "results = ner(text)\n",
    "for entity in results:\n",
    "    print(f\"{entity['word']} -> {entity['entity_group']} ({entity['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eja2fb7dj3r",
   "metadata": {},
   "source": [
    "## Question 4: Question Answering Systems\n",
    "\n",
    "### 1. What type of question answering is this? (Extractive vs. Generative)\n",
    "\n",
    "This is extractive question answering. The model finds and returns a span of text directly from the context, rather than generating new text.\n",
    "\n",
    "### 2. What do `start` and `end` indices represent? Why are they important?\n",
    "\n",
    "The `start` and `end` indices are character positions marking where the answer begins and ends in the context. They let you extract the exact answer span, highlight it in a UI, or verify where the answer came from.\n",
    "\n",
    "### 3. What is the SQuAD dataset?\n",
    "\n",
    "SQuAD (Stanford Question Answering Dataset) is a benchmark for extractive QA with over 100,000 question-answer pairs based on Wikipedia articles. Answers are always text spans from the articles. SQuAD 2.0 also includes unanswerable questions.\n",
    "\n",
    "Model card: https://huggingface.co/distilbert-base-cased-distilled-squad\n",
    "\n",
    "### 4. Question the model CANNOT answer\n",
    "\n",
    "Questions requiring inference or information not in the text will fail. For example, asking \"What is the CEO's opinion?\" when no opinion is stated, or \"What will happen next?\" since extractive QA can only return text that exists in the context.\n",
    "\n",
    "### 5. Challenge: Extractive vs Generative QA\n",
    "\n",
    "Extractive QA finds and returns exact text spans from the context. Models like BERT and DistilBERT do this. Generative QA creates new text as the answer, so it can rephrase or synthesize information. Models like T5 and BART do generative QA. The key difference is that extractive can only answer if the answer is literally in the text, while generative can produce answers even when they're not explicitly stated.\n",
    "\n",
    "For a generative example, see the code cell below using `google/flan-t5-base`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "w2dh27a0czb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: New York City\n",
      "Score: 95.34%\n",
      "Position: 90 to 103\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\")\n",
    "context = \"Hugging Face was founded in 2016 by Cl√©ment Delangue, Julien Chaumond, and Thomas Wolf in New York City.\"\n",
    "question = \"Where was Hugging Face founded?\"\n",
    "result = qa(question=question, context=context)\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Score: {result['score']:.2%}\")\n",
    "print(f\"Position: {result['start']} to {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "m59nd0oe9hf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Paris'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "result = generator(\"question: What is the capital of France? context: France is a country in Western Europe. Its capital is Paris, known for the Eiffel Tower.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98vc19r6al",
   "metadata": {},
   "source": [
    "## Question 5: Text Summarization\n",
    "\n",
    "### 1. What is the difference between extractive and abstractive summarization?\n",
    "\n",
    "Extractive summarization picks and combines existing sentences from the source text. Abstractive summarization generates new text that captures the meaning, potentially rephrasing or using different words. The default model uses abstractive summarization.\n",
    "\n",
    "### 2. Default model analysis: `sshleifer/distilbart-cnn-12-6`\n",
    "\n",
    "This is a DistilBART model (distilled version of BART) trained on CNN/DailyMail and XSum news articles. It's about 1.24x faster than BART-large-cnn with similar quality.\n",
    "\n",
    "Model card: https://huggingface.co/sshleifer/distilbart-cnn-12-6\n",
    "\n",
    "### 3. What do `max_length` and `min_length` control?\n",
    "\n",
    "`max_length` sets the maximum tokens in the summary, `min_length` sets the minimum. If `min_length` is larger than `max_length`, you get a warning and the model just generates up to `max_length`.\n",
    "\n",
    "### 4. What does `clean_up_tokenization_spaces=True` do?\n",
    "\n",
    "It removes extra spaces around punctuation that appear during tokenization. Without it you might get \"Hello , world .\" instead of \"Hello, world.\"\n",
    "\n",
    "### 5. Challenge: Two different summarization models\n",
    "\n",
    "For short texts like news articles, `facebook/bart-large-xsum` is optimized for single-sentence summaries. For long documents like research papers, `allenai/led-base-16384` can handle up to 16,384 tokens using Longformer attention, compared to ~1024 for standard BART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6gqxumodj6m",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Machine learning models can now understand natural language, recognize images, and even generate creative content . Companies are investing billions of dollars in AI research and development .\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "article = \"\"\"\n",
    "Artificial intelligence has transformed many industries in recent years.\n",
    "Machine learning models can now understand natural language, recognize images,\n",
    "and even generate creative content. Companies are investing billions of dollars\n",
    "in AI research and development. However, concerns about ethics and job displacement\n",
    "continue to be debated by experts and policymakers around the world.\n",
    "\"\"\"\n",
    "result = summarizer(article, max_length=50, min_length=20)\n",
    "print(result[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6szsgmnr6r",
   "metadata": {},
   "source": [
    "## Question 6: Machine Translation\n",
    "\n",
    "### 1. What is the architecture behind `Helsinki-NLP/opus-mt-en-de`?\n",
    "\n",
    "It uses the Marian architecture, a transformer encoder-decoder with 6 layers each. OPUS stands for Open Parallel Universal Sources (a collection of translated texts from the web), and MT stands for Machine Translation. It uses SentencePiece tokenization.\n",
    "\n",
    "Model card: https://huggingface.co/Helsinki-NLP/opus-mt-en-de\n",
    "\n",
    "### 2. How to find an English to French translation model?\n",
    "\n",
    "Two options: `Helsinki-NLP/opus-mt-en-fr` for standard translation, or `Helsinki-NLP/opus-mt-tc-big-en-fr` for higher quality. See the code cell below.\n",
    "\n",
    "### 3. Bilingual vs Multilingual translation models\n",
    "\n",
    "Bilingual models like `opus-mt-en-de` handle one language pair and are smaller, faster, and usually higher quality for that specific pair. Multilingual models like `facebook/m2m100_418M` handle many language pairs (100 languages, 9,900 directions) with one model, but trade off some quality for flexibility.\n",
    "\n",
    "### 4. How does `\"translation_en_to_de\"` relate to the model?\n",
    "\n",
    "The task name specifies the translation direction: `en` is the source language (English), `de` is the target (German). This must match what the model was trained for. For French, use `translation_en_to_fr` with `Helsinki-NLP/opus-mt-en-fr`.\n",
    "\n",
    "### 5. What is `sacremoses` used for?\n",
    "\n",
    "Sacremoses is a Python port of the Moses statistical MT toolkit. It provides tokenization, detokenization, punctuation normalization, and truecasing for text preprocessing in NLP pipelines.\n",
    "\n",
    "### 6. Challenge: Multilingual model\n",
    "\n",
    "`facebook/m2m100_418M` supports 100 languages and can translate directly between any pair without going through English as a pivot. There's also a larger 1.2B parameter version for better quality.\n",
    "\n",
    "Model card: https://huggingface.co/facebook/m2m100_418M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tytlyxyr2jl",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/othocs/Desktop/AIDAMS/Semester 1/knowledgeGraph/Knowledge_Graph_Class/NLPCLass/TP3/.venv/lib/python3.13/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Bonjour, comment allez-vous aujourd'hui ?\"}]\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "result = translator(\"Hello, how are you today?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "awcsa0s8f3m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French: Bonjour, comment allez-vous?\n",
      "German: Hallo, wie geht es dir?\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "tokenizer.src_lang = \"fr\"\n",
    "text = \"Bonjour, comment allez-vous?\"\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "generated = model.generate(**encoded, forced_bos_token_id=tokenizer.get_lang_id(\"de\"))\n",
    "result = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"French: {text}\")\n",
    "print(f\"German: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0i1ix4qyurvm",
   "metadata": {},
   "source": [
    "## Question 7: Text Generation\n",
    "\n",
    "### 1. What is the default model for text-generation? What architecture does GPT-2 use?\n",
    "\n",
    "The default model is `openai-community/gpt2`. GPT-2 uses a decoder-only transformer architecture with 124M parameters in the base version. It performs autoregressive generation, meaning it predicts the next token based on all previous tokens. Larger variants include gpt2-medium (355M), gpt2-large (774M), and gpt2-xl (1.5B).\n",
    "\n",
    "### 2. Why do we use `set_seed(42)` before generation?\n",
    "\n",
    "Setting a random seed ensures reproducibility. When using sampling-based generation (`do_sample=True`), the model randomly selects tokens from a probability distribution. Without a fixed seed, you get different outputs each run. With `set_seed(42)`, the same prompt produces the same output every time.\n",
    "\n",
    "### 3. What parameters control text generation?\n",
    "\n",
    "- `temperature`: controls randomness. Lower values (e.g., 0.7) make output more focused/deterministic, higher values (e.g., 1.5) make it more random/creative\n",
    "- `top_k`: only samples from the k most likely tokens at each step\n",
    "- `do_sample`: when False, uses greedy decoding (always picks most likely token). When True, enables sampling\n",
    "- `top_p` (nucleus sampling): samples from the smallest set of tokens whose cumulative probability exceeds p\n",
    "- `max_new_tokens`: limits how many tokens to generate\n",
    "\n",
    "### 4. What does the truncation warning mean?\n",
    "\n",
    "GPT-2 has a maximum context length of 1024 tokens. If your input prompt exceeds this, the model truncates it to fit. This means early parts of long prompts get cut off, losing context.\n",
    "\n",
    "### 5. What does `pad_token_id` being set to `eos_token_id` mean?\n",
    "\n",
    "GPT-2 wasn't trained with a padding token, so it doesn't have one defined. The pipeline automatically uses the end-of-sequence token (ID 50256) as padding. This is needed for batch processing where sequences have different lengths.\n",
    "\n",
    "### 6. What are the trade-offs between model size and generation quality?\n",
    "\n",
    "Larger models like gpt2-xl (1.5B parameters) produce more coherent and contextual text, but are slower and require more memory. Smaller models like gpt2 (124M parameters) are faster and lighter but produce lower quality output. You can balance this with techniques like distillation or quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3zgorbpitm6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " uncertain.\n",
      "\n",
      "\"We're not sure what the future will look like,\" said Dr. Michael S. Schoenfeld, a professor of computer science at the University of California, Berkeley. \"But we're not sure what the future will look\n",
      "\n",
      "Sampling (temperature=0.7):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " uncertain. In the meantime, we need to be more patient and more patient with AI.\n",
      "\n",
      "The first thing to know about artificial intelligence is that it is not new. The world has already been doing with it, with the same sort of ideas\n",
      "\n",
      "Top-k (k=50):\n",
      " uncertain, although it is an area that could be explored soon in a new way.\n",
      "\n",
      "\"It is a very, very, very, very interesting world,\" said Chris Stapel, executive director of the AI Society, a nonprofit that advises\n"
     ]
    }
   ],
   "source": [
    "generator_small = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"Greedy:\")\n",
    "print(generator_small(prompt, max_new_tokens=50, do_sample=False, return_full_text=False )[0]['generated_text'])\n",
    "\n",
    "print(\"\\nSampling (temperature=0.7):\")\n",
    "print(generator_small(prompt, max_new_tokens=50, do_sample=True, temperature=0.7, return_full_text=False)[0]['generated_text'])\n",
    "\n",
    "print(\"\\nTop-k (k=50):\")\n",
    "print(generator_small(prompt, max_new_tokens=50, do_sample=True, top_k=50, return_full_text=False)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1b293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
